# Level-Crossing Principles {#level-crossingprinciples}

## First-Order and Second-Order Epistemology {#first-orderandsecond-orderepistemology}

In the previous part I argued that morality is independent of both what one thinks about morality, and what one should think about morality. In this part I want to argue the same thing for epistemology. But we have to be a bit careful setting up the independence thesis. Informally, a thesis of the previous part was that morality and epistemology are distinct existences. Arguing 'the same thing' for epistemology would amount to arguing that epistemology and epistemology are distinct existences. That doesn't sound particularly plausible. So to state my intended conclusion a bit more carefully, and a bit more plausibly, we need one bit of terminology.

Say that a claim that either describes or evaluates a particular belief of a person is **first-order** when that very belief is not itself a description or evaluation of a particular belief. And say that a claim that either describes or evaluates a particular belief of a person is **second-order** when the belief in question is a description or evaluation of another belief. So here are some examples of first-order claims.

-   [Baba]{acronym-label="Baba" acronym-form="singular+short"} believes that his keys are missing.
-   [Baba]{acronym-label="Baba" acronym-form="singular+short"} should believe that his keys are missing.

And here are some examples of second-order claims.

-   [Baba]{acronym-label="Baba" acronym-form="singular+short"} believes that he believes that his keys are missing.
-   [Baba]{acronym-label="Baba" acronym-form="singular+short"} should believe that he believes that his keys are missing.
-   [Baba]{acronym-label="Baba" acronym-form="singular+short"} believes that he should believe that his keys are missing.
-   [Baba]{acronym-label="Baba" acronym-form="singular+short"} should believe that he should believe that his keys are missing.

And we can replace 'should' in any of these claims with any other kind of epistemic norm. So here are some more first order claims.

-   [Baba]{acronym-label="Baba" acronym-form="singular+short"}'s evidence supports the belief that his keys are missing.
-   [Baba]{acronym-label="Baba" acronym-form="singular+short"}'s belief that his keys are missing is justified.
-   [Baba]{acronym-label="Baba" acronym-form="singular+short"} rationally believes that his keys are missing.

And here are a sample of some more second-order claims.

-   [Baba]{acronym-label="Baba" acronym-form="singular+short"} should believe that he rationally believes his keys are missing.
-   [Baba]{acronym-label="Baba" acronym-form="singular+short"}'s evidence supports the belief that his belief that his keys are missing is justified.
-   [Baba]{acronym-label="Baba" acronym-form="singular+short"}'s belief that he believes that his keys are missing is justified.

The core thesis of this part of the book, the core thesis of normative externalism in epistemology, is that first-order and second-order claims are independent. There are no true **level-crossing** principles, describing necessary connections between first-order and second-order claims.

Just like in part one, there are fall-back positions I will adopt if this strong claim (no necessary connections at all) turns out to be false. If there are necessary connections, they are one-way, or they are about less central concepts in epistemology, or the explanation of the claim does not go from the second-order claim to the first-order claim. But I'd rather not retreat even to there, and instead to argue that there are no true level-crossing principles at all.

I'm interested in level-crossing principles for a few reasons. For one thing, I find them intrinsically interesting. For another, they have consequences for a bunch of epistemological debates. I'm going to discuss at the end of the book the consequences they have for disputes about how to best respond to peer disagreement. But they also matter for a bunch of other debates. And they matter because to the extent they are true, they push us towards a certain kind of coherentism, and away from a certain kind of foundationalism. Just which kind will depend on just which level-crossing principles are true. But the general idea is that if rationality requires conformity to one's own beliefs about the rational, then rationality is more of a coherence concept than we might have thought it was.

## Change Evidentialism {#changeevidentialism}

It isn't just the principles that push away from foundationalism. The examples that are used to motivate level-crossing principles are also taken to mitigate against a fairly weak form of foundationalist evidentialism that I'll call Change Evidentialism.

Change Evidentialism
:   A person with a rational attitude towards *p* is under no rational obligation to change that attitude unless their evidence for or against *p* changes.

I think Change Evidentialism is true. Indeed, I think a much stronger form of foundationalist evidentialism, one that says the rational status of a mental state supervenes on the evidence it is based on, is true. It is far beyond the scope of this book to defend the stronger claim. I will, in effect, be defending evidentialism against a class of attacks, but that defence will not be the focus.

Change Evidentialism is related to theses level-crossing principles because some cases that motivate the principles also appear to undermine Change Evidentialism. Here is one such case, due to David Christensen.

> I'm a medical resident who diagnoses patients and prescribed appropriate treatment. After diagnosing a particular patient's condition and prescribing certain medications, I'm informed by a nurse that I've been awake for 36 hours. Knowing what I do about people's propensities to make cognitive errors when sleep-deprived (or perhaps even knowing my own poor diagnostic track-record under such circumstances), I reduce my confidence in my diagnosis and prescription, pending a careful recheck of my thinking.  [@Christensen2010a 186].

We might naturally reason about the case as follows. (Note this isn't Christensen's own considered take on the case.) When the resident learns he has been awake 36 hours, he does not get evidence against the diagnosis. That a particular resident has been awake awhile seems evidentially irrelevant to whether a particular patient has, let's say, dengue fever. But it is rational, indeed it is rationally required, for the resident to change his attitude towards the diagnosis on learning how long he's been awake. That's a counterexample to Change Evidentialism. And the explanation for why rationality requires a change is, we might conjecture, that principle 1 is true. The resident does have evidence excellent that he's making irrational diagnoses. So he can't rationally believe that he rationally believes the diagnosis. So, by the contrapositive of 1, he can't rationally believe the diagnosis.

I'm going to argue that the previous paragraph is all false. What the resident should do depends a lot on the details of the case. On some ways of filling in the case, the resident's evidence changes substantially, so Change Evidentialism is consistent with the resident rationally changing their view. Indeed, the explanation of the change of view in terms of change of evidence is preference to the explanation in terms of a level crossing principle like 1. That's because in other versions of the case, where the resident's evidence does not change, the belief in the diagnosis should not change either.

So I'm going to argue that cases like Christensen's resident not only fail to challenge Change Evidentialism, they end up supporting it. And because the cases support it, they don't support 1--4. There are also direct counterexamples to 1--4. The example in the next chapter of [Roshni]{acronym-label="Roshni" acronym-form="singular+short"} is one such counterexample.

## Motivations for Level-Crossing {#motivationsforlevel-crossing}

The rest of this book will be devoted to investigating three recent motivations for level-crossing principles. The first concerns higher-order evidence, the second akrasia, and the third peer disagreement.

### Higher-Order Evidence {#higher-orderevidence}

As well as evidence that bears on a question, agents can have evidence that bears on the rationality of their verdicts about the question. Christensen's example involving the medical resident is one such case. Elsewhere, Christensen has provided several other examples along similar lines, e.g.,  [@Christensen2007a 8],  [@Christensen2010b 126] and  [@Christensen2011 5-6]. Similar examples have also been proposed by Adam @Elga2008, Thomas @Kelly2010 [140], Joshua @Schechter2013 [443-4] and Sophie @Horowitz2014 [719]. The examples suggest something like the following argument against Change Evidentialism.

1.  It is irrational for the resident in this case to stick with the original prescription without making some kind of cross-check.
2.  The best explanation of why it is irrational to stick with the original prescription is that it is irrational to stick with the original diagnosis, i.e., the original belief.
3.  The information the nurse provides is not evidence one way or the other about whether the patient has the disease originally diagnosed.
4.  It was rational, before the information the nurse provides about how long the resident has been awake, to believe in the original diagnosis.
5.  So this is a case where the rationality of a belief changes without any change in the evidence.

The last line follows from what came before, so the issue is whether the first four claims are true. I'm going to raise doubts about every one of those steps. But this is, I think, the most pressing challenge to Change Evidentialism.

### Akrasia {#akrasia}

Assume, for reductio, that the level-crossing principles are false. And assume that in any field, it is possible to have evidence that supports being extremely confident in something that is, as a matter of fact, false. Then there should be cases where one's evidence strongly supports *p*, but one's evidence also strongly supports the falsehood that one has very poor evidence for *p*. If one follows the evidence where it leads, one should be very confident in is the conjunction *p, and I have very weak evidence for p*. Assuming one believes (correctly!) that it is rational to follow the evidence where it leads, one should believe the conjunction: *p, and it is irrational for me to be confident in p*. But it is absurd to think that one can rationally be confident in either of these conjunctions; they are instances of epistemic akrasia, and akrasia is paradigmatically irrational.

I'm going to come back to this argument in chapter 10. The main response will be that the apparent absurdity is really not that absurd. Indeed, the intuition that it is absurd can be shown to be highly unreliable; it supports the 'absurdity' of many things that are plainly true. For now, note the connection between intuitions about akrasia and intuitions about Christensen's resident case. If the resident follows the evidence where it leads, he'll believe that the diagnosis is correct, and this belief is irrational. It looks like Evidentialism, and perhaps just Change Evidentialism, implies that the resident should be akratic. Unlike many philosophers, I won't take this to be a decisive objection to Change Evidentialism.

### Disagreement {#disagreement}

It seems possible for people who are known to have equally good track records, and who in some sense have the same evidence, to come to different conclusions. When they do, there is something intuitively plausible about each moving their beliefs in the direction of the other. Here is one such case.

> [Ankita]{acronym-label="Ankita" acronym-form="singular+short"} and [Bojan]{acronym-label="Bojan" acronym-form="singular+short"} have known each other for a long time, and know each other to be equally reliable, and equally reasonable, when it comes to arithmetic problems about as complex as multiplying two two-digit numbers. For some practical purpose they need to know what 22 times 18 is. They each do the multiplication quickly in their head. [Ankita]{acronym-label="Ankita" acronym-form="singular+short"} announces that she got 396, while [Bojan]{acronym-label="Bojan" acronym-form="singular+short"} announces that he got 386. (Compare a similar case in @Christensen2007c [193].)

Again, we can use the case to construct an argument against Change Evidentialism, as follows.

1.  [Ankita]{acronym-label="Ankita" acronym-form="singular+short"}'s original evidence provides her strong reason to believe that 22 times 18 is 396.
2.  [Bojan]{acronym-label="Bojan" acronym-form="singular+short"}'s announcement is no evidence against the claim that 22 times 18 is 396. 
3.  Yet, on hearing [Bojan]{acronym-label="Bojan" acronym-form="singular+short"}'s announcement, and respecting the fact that the two of them have equally good track records, [Ankita]{acronym-label="Ankita" acronym-form="singular+short"} should be unsure which of them is right, and which wrong, on this occasion.
4.  Since [Ankita]{acronym-label="Ankita" acronym-form="singular+short"} knows what each of them announced, the only way she can consistently be unsure which of them is right is to be unsure whether 22 times 18 is 396.
5.  So although [Bojan]{acronym-label="Bojan" acronym-form="singular+short"}'s announcement does not change her evidence that bears on whether 22 times 18 is 396, it does change whether it is rational for her to fully believe that this is true.

Again, this looks like a reasonably intuitive argument against Change Evidentialism. And again, I'm going to raise doubts about every premise. The focus of chapter 12 will be the picture of disagreement behind premise 3. This is the view that has come to be called *conciliationism*. But what I say about evidence over the next few chapters will also raise concerns about the first two premises of the argument.

## The Plan for the Rest of the Book {#theplanfortherestofthebook}

In what follows, the even-numbered chapters will deal with the three big arguments for level-crossing principles, and against Change Evidentialism, that I just discussed. In chapter 8, I'll discuss higher-order evidence; in chapter 10, I'll discuss akrasia principles, and in chapter 12, I'll discuss disagreement. In between I'll address two big issues that arise out of those discussions.

In chapter 9, I'll talk about what it means for some reasoning to be problematically circular. This turns out to matter to our purposes because of a potential bit of circular reasoning that is, according to my view, perfectly acceptable. In particular, in some cases where there is reason to believe the agent is incapable of correct reasoning, I think it is possible for the agent to simply do some correct reasoning, notice that it is correct, and infer that they are, after all, capable. This can feel worryingly circular. But it turns out to be incredibly hard to find an anti-circularity principle that is both true, and violated by this reasoning.

In chapter 11, I discuss how level-crossing principles lead to nasty regresses. More precisely, I argue that level-crossing principles are only motivated if one accepts a particular assumption concerning evidential screening. And that assumption, I argue, leads to nasty regresses. The regress arguments here are similar to the regress arguments in chapter 2 against very strong level-crossing principles in ethics.

And in chapter 13, I briefly summarise the lessons of both the epistemology part, and of the book as a whole.

But before we get to all that, I need to do a little ground-clearing. The rest of this chapter contains two fairly self-contained sections on things I wanted to get out of the way before defending Change Evidentialism against level-crossing principles. The next section concerns the relationship between state-level evaluations, like the rationality of a belief, and agent-level evaluations, like the wisdom of a believer. And then I argue, against most orthodox wisdom in epistemology, that we acquire evidence while doing mathematical investigation.

These two sections are helpful for understanding the rest of the book. But they are not essential. And someone who is impatient to get on to higher-order evidence, akrasia or disagreement could skip ahead to any one of those chapters.

## Evidence, Rationality and Wisdom {#evidencerationalityandwisdom}

Change Evidentialism is a claim about the rationality of beliefs and other doxastic attitudes. The level-crossing principles I reject are principles about evidence, and about rationality. The focus here, as you may have gathered, is on rationality and on evidence. There are other concepts in the area that I don't have as much to say about, and which may not be systematically related to those concepts.

I don't want to assume that a belief is rational if and only if it is justified. It might be that only true beliefs are justified  [@Littlejohn2012], but it is very unlikely that only true beliefs are rational.[^39] In any case, there is something a little artificial about talking about justified beliefs. In everyday English, it is typically actions that are justified or not. The justification of belief seems a somewhat derivative notion. So I'll stick to rationality.

[^39]: That false beliefs can be rational seems more plausible to me than the premises of any argument I could give for it. But here is one independent way to make the case. Arbitrarily high credences in false propositions can be rational. Indeed, false propositions can have arbitrarily high objective chances, consistent with those chances being known. In such cases the only rational credence matches the chance. The best theories of the relationship between credence and chance do not require credence 1 for belief simpliciter  [@Weatherson2014]. And if a high credence constitutes a belief, and the credence is rational, the belief is rational. So some false beliefs can be rational.

I'm also going to set aside, for the most part, a discussion of wisdom. Just as in the discussion of ethics, it is very important to keep evaluations of agents apart from evaluations of acts or states. It is attitudes or states that are in the first instance rational or irrational. We can talk about rational or irrational agents, but such notions are derivative. Rational agents are those generally disposed to have rational attitudes, and to be in rational states. Wisdom, on the other hand, is in the first instance a property of agents. Again, we can generalise the term to attitudes or states. A wise decision, for instance, is one that a wise person would make. But the wisdom of agents is explanatorily and analytically prior to the wisdom of their acts, judgments, decisions and attitudes.

I think that everything I said in the last paragraph is true if we use 'wise' and 'rational' and their cognates with their ordinary meaning. But I'm not committed to that, and it doesn't matter if I'm wrong. You can read me as stipulating that 'rational' is to be used as a term that in the first instance applies to states, and 'wise' is to be used as a term that in the first instance applies to agents, and little will be lost.

Change Evidentialism is not a claim about wise agents, it is a claim about the rationality of various beliefs and belief transitions. Perhaps a wise agent is one who always has rational attitudes. If so, then Change Evidentialism will have some implications for what wise agents are like. But it is far from obvious that wisdom and rationality are this tightly linked. Indeed, at the end of chapter 11, I'll come back to a reason to question the connection. For all I've said, it may well be wise to change one's beliefs in some situations where one's evidence does not change. That is consistent with Change Evidentialism, provided we understand those situations as being ones where it is unwise to have rational attitudes.

I am leaning heavily her on work on the connection between rationality and wisdom by Maria [@Lasonen-Aarnio2010b; @Lasonen-Aarnio2014]. I agree with almost everything she says about the connection. The biggest difference between us is terminological. She uses 'reasonable' and 'reasonableness' where I use 'wise' and 'wisdom'. In my idiolect, I find it too easy to confuse 'rational' and 'reasonable'. So I'm using a different term, and one that, to me at least, more strongly suggests a focus on agents not states. But this is a small point, and everything I say about the distinction draws heavily on Lasonen-Aarnio's work.

## Evidence, Thought and Mathematics {#evidencethoughtandmathematics}

The picture of evidence behind the version of evidentialism that I'm presenting here differs from a natural picture that many epistemologists have. In particular, I draw the line between acquiring evidence and processing evidence at a very different place than many others do. I'm going to motivate this re-drawing by working through some examples involving mathematics. Much of what I say about these examples follows closely the arguments that Paul @Boghossian2003 made against simple forms of reliabilism and internalism about logic and mathematics. But these arguments of Boghossian's are worth rehearsing, because their significance for recent epistemological debates has not always been appreciated.

A young mathematics student, [Tamati]{acronym-label="Tamati" acronym-form="singular+short"}, starts thinking about primes. He notices the gaps between primes get larger, and starts to wonder whether there is a largest prime. He is struck by a sudden strong conviction that there is no largest prime, and so forms the belief that there is no largest prime. Now [Tamati]{acronym-label="Tamati" acronym-form="singular+short"} is not usually prone to forming beliefs on the basis of spontaneous convinctions like this. Apart from this time, he only does this for very simpile arithmetic claims, like that seven plus five is twelve. But nor is he a mathematical savant. He couldn't produce any reason for the claim that there is no largest prime. He hasn't seen, even implicitly, anything like the argument that if *n* is the largest prime, then *n* + 1 would be both prime and not prime. It's just an immediate convinction for him.

[Tamati]{acronym-label="Tamati" acronym-form="singular+short"} does not know that there is no largest prime. This fact, assuming it is a fact, needs explaining. The evidentialist has a natural explanation. In a normal case, when someone comes to learn by proof that there is no largest prime, there are two extra facts they learn. The first is that if *n* is the largest prime, then *n*! + 1 is prime; the second is that if *n* is the largest prime, then *n*! + 1 is not prime. These in turn aren't immediately obvious; to be known they must be figured out on the basis of other things. Those extra pieces of knowledge are extra evidence[^40] It could be that the extra evidence would just be the premises that he would use, not the conclusions he draws from them. Or it could be that we need to give up the idea that evidence is non-inferential. I don't have a worked. It is with that evidence that a normal student can come to know, by proof, that there is no largest prime. Alternatively, the student may learn that some teacher, or some book, says that there is no largest prime, and that teacher, or book, is reliable. Those things are the extra evidence. That case is clearly different to [Tamati]{acronym-label="Tamati" acronym-form="singular+short"}'s, because it relies on engagement with the outside world. But even the student who thinks through the case themselves acquires evidence, namely the above facts about the relationship between *n* and *n*! + 1.

[^40]: Is this consistent with my earlier note that we would, for the sake of discussion, identify evidence with non-inferential knowledge? It isn't obvious that it is, and it is more than a little tricky to say just what evidence [Tamati]{acronym-label="Tamati" acronym-form="singular+short"} would gain if he worked through the problem carefully. It would work to defend normative externalism if we identified evidence with all knowledge, as Williamson suggests, but I would rather not make that identification on other grounds. I hope to return to the question of just how we should conceptualise evidence, both in mathematical and empirical investigations, in subsequent work.

So the evidentialist has a nice explanation of what is going on in [Tamati]{acronym-label="Tamati" acronym-form="singular+short"}'s case. Other explanations look less promising.

We could try to explain [Tamati]{acronym-label="Tamati" acronym-form="singular+short"}'s case in strictly reliabilist terms. But note that [Tamati]{acronym-label="Tamati" acronym-form="singular+short"}'s convictions are perfectly reliable. The method 'trust my convinctions' gets him arithmetic knowledge every day, and the true belief that there is no largest prime. In no case does it go wrong. So the reliabilist has no explanation of why this use of [Tamati]{acronym-label="Tamati" acronym-form="singular+short"}'s convictions does not yield knowledge. The reliabilist could try to argue that methods have to be individuated more finely than this; it is different to trust one's convictions about simple matters as compared to more complex matters. But this assumes we have some grasp on the idea that saying there is no largest prime is a complex matter. It isn't clear why this should be so. It isn't hard to state the proposition that there is no largest prime. It is a little hard to prove it. The evidentialist has an explanation of why how hard it is to prove the theorem matters to whether [Tamati]{acronym-label="Tamati" acronym-form="singular+short"} can spontaneously know it. But it seems very hard to motivate the idea that proof complexity should define the relevant reference class. It seems to use the very thing we were trying to give a reliabilist explanation of. In any case, even if we restrict the reference class to things that are hard to prove, [Tamati]{acronym-label="Tamati" acronym-form="singular+short"}'s convictions are still reliable. He sensibly declines to form beliefs about most things in this class, while forming one true belief. So he's got a perfect success rate, so is reliable!

Alternatively, we could say that [Tamati]{acronym-label="Tamati" acronym-form="singular+short"} doesn't "appreciate" the evidence for the absence of a largest prime. (The idea that appreciating the evidence is important to mathematical knowledge comes from Richard @Fumerton2010, though he doesn't use it for quite this purpose, and shouldn't be thought responsible for the view I'm about to criticise.) The thought would be that [Tamati]{acronym-label="Tamati" acronym-form="singular+short"} has some evidence about primes, but doesn't stand in the special relationship to it needed to ground knowledge. This is obviously an anti-evidentialist position, since it says that rationality depends not just on what evidence one has, but on some further relationship that one may or may not stand in to evidence.

But depending on how we understand 'appreciate', the view will be too strong or too weak. If appreciation means understanding how and why the evidence supports the conclusion, and appreciation is required for knowledge, then very few people will know very much. Before they take a logic class, introductory students can come to know *Ga* by inferring it from *Fa* and ∀*x*(*Fx* → *Gx*). But they don't need to know how or why their evidence supports *Ga*. Indeed, they can be radically mistaken about the nature of logical implication, as many students are, and still know *Ga* on that basis. On the other hand, if appreciation means having a true belief that the evidence supports the conclusion, it won't rule out [Tamati]{acronym-label="Tamati" acronym-form="singular+short"} knowing that there is no largest prime. We can assume that [Tamati]{acronym-label="Tamati" acronym-form="singular+short"} is sophisticated enough to know that mathematical truths are entailed by any proposition. So if he believes there is no largest prime, he can immediately (and correctly) infer that the fact that his coffee has gone cold entails there is no largest prime. But that isn't enough for him to know there is no largest prime, not even if he knows that his coffee has gone cold. If we insist that appreciation means knowing that the evidence supports the conclusion, then we are back where we started, needing to explain why [Tamati]{acronym-label="Tamati" acronym-form="singular+short"} doesn't know that there is no largest prime.

So the best explanation of [Tamati]{acronym-label="Tamati" acronym-form="singular+short"}'s ignorance is that he lacks sufficient evidence to know that there is no largest prime. If he worked through the problem slowly, he would acquire evidence for that conclusion. And that's the general case. Thinking through a mathematical problem involves acquiring mathematical evidence. Similarly, when one has to do some mathematical reasoning to get from empirical data to empirical conclusion, that reasoning doesn't just involve processing the empirical evidence, it involves acquiring new, mathematical evidence.

This way of thinking about mathematics is hardly radical. It is a commonplace in mathematical discussions that one can get evidence for or against mathematical propositions. Philosophers too often think that evidence that entails a conclusion is maximally strong evidence. This assumption is even encoded into probabilistic models of evidential support. But it isn't true. Facts about Andrew Wiles's diet are terrible evidence that Fermat's Last Theorem is true, although they entail it. Fact about what he wrote in his notebooks, on the other hand, are excellent evidence that it is true. Thinking that entailing reasons are maximally strong reasons is just another way to confuse inference with implication  [@Harman1986].

This attitude, of thinking that entailing reasons are maximally strong reasons, goes along with another bad attitude that it is easy to adopt. That is the attitude that when *p* is a mathematical proposition, our evidence supports either a maximally strong belief in *p* or a maximally strong belief in ¬*p*. There are numerous counterexamples to this view. Sanjoy @Mahajan2010 describes heuristics that can be used to quickly refute various mathematical hypotheses. The heuristics involve, for example, checking whether the 'dimensions' of a proposed identity are correct, and checking limit cases. So consider the hypothesis that the area of an ellipse is π*ab*, where *a* is the distance from the centre of the nearest point on the ellipse, and *b* is the distance from the centre to the furthest point. After going through a number of other proposals and showing how they can be quickly refued, Mahajan says this about the proposal that the area is π*ab*,

> This candidate passes all three tests ... With every test that a candidate passes, confidence in it increases. So you can be confident in this candidate. And indeed it is correct.  [@Mahajan2010 21]

It might be worried that the position I'm adopting here, that we often need evidence of a connection between premises and conclusion in order to reasonably infer the conclusion from the premises, even when the premises entail the conclusion, risks running into the regresses described by Lewis @Carroll1895. It certainly would be bad if my view implies that to infer *q* from *p* and *p* → *q*, and agent needed to know (*p* ∧ (*p* → *q*)) → *q*. That way lies regress, and perhaps madness. But that's not what my view implies. The claim is just that for non-obvious entailments, the agent needs extra knowledge to infer from premises to conclusions. It is consistent with this to say that immediate entailments, like modus ponens, can justify immediate inferences. And that's enough to stop the regress.

The idea that we accumulate evidence when working through philosophical or mathematical puzzles will matter quite a bit for debates about disagreement. It is agreed on all sides that when the parties to a disagreement do not have the same evidence, then the existence of the disagreement is a reason for each to move their attitudes. (Assuming, of course, that the other person is not irrational, or known to be bad at processing this kind of evidence.) If we allow that there is philosophical evidence, then it will be incredibly rare that each party to a debate has the same evidence. It will be vanishingly rare that each party knows that each party has the same evidence. This means that any case where the parties know about the evidence the other parties have will be a fair way removed from the kind of real-world case where we have reliable intuitions. It also means that in practice learning about the existence of a people who disagree with you is often evidence that there is evidence against your view that they have and you lack.

The main claim I'll need in what follows is that thinking through a case sometimes gives you evidence. But it's independently interesting to think how far this extends; to think about how much reasoning is a form of evidence acquisition. And examples with the same structure as [Tamati]{acronym-label="Tamati" acronym-form="singular+short"}'s can be used to motivate the thought that very often reasoning involves evidence acquisition.

A, B and C are trying to figure out how many socks are in the drawer. They each know there are seven green socks, and five blue socks, and that that's all the socks, and that no sock is both green and blue. From this information, they all infer, and come to know, that there are seven plus five socks in the drawer. A is an adult with statistically normal arithmetic skills, so she quickly infers that there are twelve socks in the drawer. B is a three year old child, who is completely unreliable at arithmetic. She guesses that there are twelve. At this stage, we can say that A knows there are twelve socks in the drawer, and B does not know it. But C, who is four years old, is a more subtle case. She says to herself, "I think it's twelve, but I better check." That's a good reaction; like B she isn't so reliable that she can know without checking. So she uses a method for doing addition; she starts counting from seven, putting one finger up at each count. So she says "eight" and raises her thumb, "nine" and raises her index finger, and so on through saying "twelve" and raising her little finger. She looks at her hand, sees that she has five fingers raised, and concludes the answer is twelve.

At the end of this process, but not before, she knows that there are twelve socks in the drawer. Indeed, it is only at the end of the process that she is in a position to know that there are twelve socks in the drawer. It is because she has come to know that seven plus five is twelve that she has sufficient evidence to know there are twelve socks in the drawer. Previously, this was not part of her evidence, now it is, and now she can know there are twelve socks in the drawer. That suggests it is because A knows that seven plus five is twelve that she can know there are twelve socks in the drawer to. She might not have consciously said to herself that seven plus five is twelve, but if she didn't have that as part of her evidence, she wouldn't have been in a position to know that there were twelve socks in the drawer. C also knows this, because she acquired this evidence. Indeed, she acquired it a posteriori; it in part relied on seeing that she had five fingers raised.

So even reasoning that relies on simple arithmetical identities relies on those identities being in evidence. In these cases, the only rule of implication that really seems to do double duty as a rule of inference is the transitivity of identity. An agent who knows that *x* equals seven plus five, and knows that seven plus five equals twelve, is in a position to infer that *x* equals twelve. They don't, it seems, need to know that identity is a transitive relationship. Whether we grant C knowledge that there are twelve socks in the drawer does not, it seems, depend on whether we grant her knowledge of the fully general principle that identity is transitive.

What's special about that last step is that the general principle that might be relevant is considerably more complicated to state, and to believe, than the general principle in arithmetic cases. It is easier to know that seven plus five is twelve than it is to know exactly what rule about identity that A, B and C need to use to figure out how many socks there are in the drawer. It is harder to know the general principle of disjunctive syllogism than it is to use it on an occasion. So it might be that there are more kinds of simple inferential steps that track simple implicative rules than there are kinds of simple inferential steps that track simple arithmetic identities. For all I've said here, it might be that all arithmetic inferences just involve the transitivity of identity, plus knowledge of a lot of arithmetic facts. It isn't so plausible that all logical inferences just involve one rule, such as modus ponens, plus a lot of logical facts.
